好的，我们已经成功搭建了Transformer的“左半边”——编码器。现在是时候构建它的“右半边”——**解码器（Decoder）**了。解码器的任务是利用编码器提供的上下文信息（我们称之为`memory`），生成目标序列。

---

### **第五部分：解码器层与完整解码器**

#### **思想过程**

1.  **解码器层的独特之处**
    *   我首先要仔细研究论文中解码器层的结构图。它和编码器层很像，但多了一个组件。一个解码器层包含**三个**子层，而不仅仅是两个。
    *   **子层一：带掩码的多头自注意力（Masked Multi-Head Self-Attention）**
        *   **目的**：让解码器关注到目前已经生成的目标序列部分。例如，在翻译“I am a student”时，如果已经生成了“我 是”，那么在预测下一个词时，模型需要回顾“我 是”这两个词。
        *   **关键点：掩码（Masking）**。这是一个“因果掩码”或“先行掩码”（look-ahead mask）。它是一个下三角矩阵，确保在计算位置 `i` 的注意力时，只能访问到 `0` 到 `i` 的位置，而不能“看到”未来的位置（`i+1`, `i+2`, ...）。这是解码器自回归（auto-regressive）性质的核心保证。我们之前在测试`scaled_dot_product_attention`时已经实现过这种掩码，现在要正式用上了。
    *   **子层二：交叉注意力（Cross-Attention or Encoder-Decoder Attention）**
        *   **目的**：这是连接编码器和解码器的桥梁。它让解码器能够“审视”整个输入源序列，并根据当前解码状态决定应该重点关注源序列的哪个部分。
        *   **Q, K, V的来源**：这是它和自注意力的最大区别。
            *   `Query (Q)`: 来自于解码器前一个子层（带掩码的自注意力）的输出。它代表了“我当前需要什么信息来生成下一个词？”。
            *   `Key (K)` 和 `Value (V)`: 都来自于**编码器的最终输出 `memory`**。`memory`包含了整个源序列的丰富上下文表示。解码器用它的`Q`去查询`memory`中的`K`，找到最相关的部分，然后从对应的`V`中提取信息。
    *   **子层三：前馈网络（Position-wise Feed-Forward Network）**
        *   这和编码器中的FFN完全一样，作用也是引入非线性，对交叉注意力的输出进行进一步的特征变换。

2.  **实现解码器层 (`DecoderLayer`)**
    *   基于以上分析，`DecoderLayer`的`__init__`需要实例化：
        *   一个`MultiHeadAttention`用于带掩码的自注意力。
        *   另一个`MultiHeadAttention`用于交叉注意力。
        *   一个`PositionwiseFeedForward`模块。
        *   **三个**`nn.LayerNorm`和**三个**`nn.Dropout`模块，每个子层后面各一个。
    *   `forward`方法的参数会比`EncoderLayer`更复杂。它需要接收：
        *   `tgt`: 目标序列的嵌入。
        *   `memory`: 编码器的输出。
        *   `tgt_mask`: 目标序列的因果掩码。
        *   `src_mask`: 源序列的填充掩码（用于交叉注意力，确保解码器不会关注到源序列中的padding部分）。
    *   `forward`的数据流将是：
        1.  `tgt`通过带`tgt_mask`的自注意力，然后`Add & Norm`。
        2.  上一步的输出作为`Q`，`memory`作为`K`和`V`，通过带`src_mask`的交叉注意力，然后`Add & Norm`。
        3.  上一步的输出通过FFN，然后`Add & Norm`。
        4.  返回最终结果。

3.  **组装完整解码器 (`Decoder`)**
    *   这和组装`Encoder`的过程非常相似。
    *   `__init__`:
        *   目标词汇表的`nn.Embedding`层。
        *   `PositionalEncoding`层。
        *   一个`nn.ModuleList`来堆叠`N`个我们刚刚设计的`DecoderLayer`。
        *   一个最终的`nn.LayerNorm`。
    *   `forward`方法接收`tgt`（目标序列ID），`memory`（编码器输出），`tgt_mask`和`src_mask`。
    *   它的流程是：`tgt` IDs -> Embedding -> Positional Encoding -> N x DecoderLayers -> Final Norm -> Output。

#### **为什么这么实现**

*   **结构清晰**：将`DecoderLayer`作为一个独立的、可复用的单元，使得整体`Decoder`的实现变得简单明了，就是对`DecoderLayer`的堆叠。
*   **功能分离**：`DecoderLayer`内部的三个子层各司其职。第一个自注意力负责理解已生成的部分，第二个交叉注意力负责从源序列获取信息，第三个FFN负责特征提炼。这种设计非常优雅且功能强大。
*   **掩码处理的严谨性**：在实现中，我需要特别注意两种掩码的正确传递和使用。`tgt_mask`用于解码器自注意力，防止未来信息泄露；`src_mask`用于交叉注意力，防止关注源句子的填充部分。这是解码器正确工作的关键。

#### **代码实现 (Part 5: Decoder Layer and the Full Decoder)**

```python
import torch
import torch.nn as nn
import math

# --- 从前面部分复制过来的模块 ---
# (为了简洁，这里省略了大部分之前已定义的模块的代码)
# 我们需要 MultiHeadAttention, PositionwiseFeedForward, 和 PositionalEncoding
# 它们的定义与之前完全相同。
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.w_2(self.dropout(self.relu(self.w_1(x))))

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model, self.n_heads, self.d_k = d_model, n_heads, d_model // n_heads
        self.w_q, self.w_k, self.w_v, self.w_o = (nn.Linear(d_model, d_model) for _ in range(4))
        self.dropout = nn.Dropout(dropout) # 这个dropout在原论文中是在Add&Norm之前
    def forward(self, q, k, v, mask=None):
        bs = q.size(0)
        q, k, v = [l(x).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2) for l, x in zip((self.w_q, self.w_k, self.w_v), (q, k, v))]
        def scaled_dot_product_attention(q, k, v, mask=None):
            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
            if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)
            p_attn = torch.softmax(scores, dim=-1)
            return torch.matmul(p_attn, v), p_attn
        context, attn = scaled_dot_product_attention(q, k, v, mask)
        context = context.transpose(1, 2).contiguous().view(bs, -1, self.d_model)
        return self.w_o(context), attn

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)
# --------------------------------------------------------------------------------

# --------------------------------------------------------------------------------
# Part 5.1: Decoder Layer
# --------------------------------------------------------------------------------
class DecoderLayer(nn.Module):
    """
    解码器层。由三个子层构成。
    思想过程：
    1.  第一个子层是带掩码的多头自注意力。
    2.  第二个子层是交叉注意力，Q来自解码器，K,V来自编码器。
    3.  第三个子层是前馈网络。
    4.  每个子层后都有残差连接和层归一化。
    """
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, 
                tgt: torch.Tensor, 
                memory: torch.Tensor, 
                tgt_mask: torch.Tensor, 
                src_mask: torch.Tensor):
        # tgt: [B, T_seq, D_m], memory: [B, S_seq, D_m]
        # tgt_mask: [B, 1, T_seq, T_seq], src_mask: [B, 1, 1, S_seq]

        # 1. 带掩码的多头自注意力 (Masked Self-Attention)
        attn_output, _ = self.self_attn(q=tgt, k=tgt, v=tgt, mask=tgt_mask)
        tgt = self.norm1(tgt + self.dropout1(attn_output))

        # 2. 交叉注意力 (Cross-Attention)
        # Q是上一层的输出tgt, K和V是编码器的memory
        cross_attn_output, cross_attn_weights = self.cross_attn(q=tgt, k=memory, v=memory, mask=src_mask)
        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))

        # 3. 前馈网络 (Feed-Forward Network)
        ffn_output = self.feed_forward(tgt)
        tgt = self.norm3(tgt + self.dropout3(ffn_output))

        return tgt

# --------------------------------------------------------------------------------
# Part 5.2: The Full Decoder
# --------------------------------------------------------------------------------
class Decoder(nn.Module):
    """
    完整的解码器，由 N 个 DecoderLayer 堆叠而成。
    """
    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, d_ff: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)
        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, 
                tgt: torch.Tensor, 
                memory: torch.Tensor, 
                tgt_mask: torch.Tensor, 
                src_mask: torch.Tensor) -> torch.Tensor:
        # tgt: [B, T_seq], memory: [B, S_seq, D_m]
        # tgt_mask: [B, 1, T_seq, T_seq], src_mask: [B, 1, 1, S_seq]
        
        # 1. 词嵌入和位置编码
        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)
        tgt_pos = self.pos_encoder(tgt_emb)
        
        # 2. 依次通过 N 个解码器层
        output = tgt_pos
        for layer in self.layers:
            output = layer(output, memory, tgt_mask, src_mask)
        
        # 3. 最终的层归一化
        output = self.norm(output)
            
        return output

# --- 测试一下 ---
if __name__ == '__main__':
    # 为了测试Decoder，我们需要一个Encoder的模拟输出来作为memory
    class Encoder(nn.Module):
        def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, d_model)
            self.layers = nn.ModuleList([nn.Identity() for _ in range(n_layers)]) # 简化版
        def forward(self, src, src_mask):
            return self.embedding(src)

    print("--- Part 5: Testing Decoder ---")
    # 假设参数
    src_vocab_size, tgt_vocab_size = 1000, 1200
    d_model = 512
    n_layers = 6
    n_heads = 8
    d_ff = 2048
    batch_size = 4
    src_seq_len, tgt_seq_len = 20, 18
    
    # 模拟编码器和输入
    encoder = Encoder(src_vocab_size, d_model, n_layers, n_heads, d_ff, 0.1)
    src_tokens = torch.randint(1, src_vocab_size, (batch_size, src_seq_len))
    src_mask = (src_tokens != 0).unsqueeze(1).unsqueeze(2) # [B, 1, 1, S_seq]
    memory = encoder(src_tokens, src_mask)
    
    # 创建解码器实例
    decoder = Decoder(vocab_size=tgt_vocab_size, d_model=d_model, n_layers=n_layers, n_heads=n_heads, d_ff=d_ff)
    
    # 创建解码器输入和掩码
    tgt_tokens = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len))
    
    # 目标序列的padding mask
    tgt_pad_mask = (tgt_tokens != 0).unsqueeze(1).unsqueeze(2) # [B, 1, 1, T_seq]
    # 目标序列的因果掩码 (look-ahead mask)
    look_ahead_mask = torch.tril(torch.ones(tgt_seq_len, tgt_seq_len)).bool().unsqueeze(0).unsqueeze(0) # [1, 1, T_seq, T_seq]
    # 合并两种掩码
    tgt_mask = tgt_pad_mask & look_ahead_mask
    
    print("Memory shape from Encoder:", memory.shape)
    print("Target tokens shape:", tgt_tokens.shape)
    print("Target mask shape:", tgt_mask.shape)
    
    # 前向传播
    output = decoder(tgt_tokens, memory, tgt_mask, src_mask)
    
    print("Decoder output shape:", output.shape)
    
    # 检查输出维度
    assert output.shape == (batch_size, tgt_seq_len, d_model)
    
    print("\nDecoder test passed!")
```

#### **总结与下一步**

至此，我们已经成功地构建了Transformer模型的所有核心组件：编码器（Encoder）和解码器（Decoder）。我们已经非常接近终点线了。

我们现在有一个`Encoder`可以创建源句子的表示，还有一个`Decoder`可以根据这个表示和已生成的目标词来生成下一个词的表示。但`Decoder`的输出是一个`[batch_size, seq_len, d_model]`的浮点数张量，我们如何把它变成一个具体的词（或者说，下一个最可能的词的概率分布）呢？

这就需要最后一步：
1.  **添加一个最终的线性层（Linear Layer）**，它将解码器的输出映射到目标词汇表的大小。
2.  **应用Softmax函数**，将线性层的输出（logits）转换成一个概率分布。
3.  **将Encoder和Decoder组合起来**，形成一个完整的、端到端的`Transformer`模型。

**因此，我的下一步是：将Encoder和Decoder封装到一个顶层`Transformer`类中，并添加最后的输出层，完成整个模型的构建。**

我将在最后一次回答中完成这个过程。
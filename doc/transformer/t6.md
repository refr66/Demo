好的，我们已经走到了最后一步。我们已经精心打造了编码器和解码器这两个强大的组件。现在，是时候将它们组装在一起，并加上画龙点睛的最后一笔，形成一个完整、可运行的**Transformer**模型。

---

### **第六部分：组装最终的Transformer模型**

#### **思想过程**

1.  **回顾全局架构**
    *   我脑海中浮现出论文《Attention Is All You Need》的整体架构图。左边是编码器栈，右边是解码器栈。编码器的输出（`memory`）是解码器每个层交叉注意力的关键输入。
    *   整个模型的工作流程是：
        1.  源语言句子（token IDs）进入编码器，生成上下文表示 `memory`。
        2.  目标语言句子（token IDs，在训练时是真实的目标句，在推理时是已生成的词）进入解码器。
        3.  解码器利用 `memory` 和自身状态，生成一系列高维表示。
        4.  这些高维表示通过一个最终的线性层，被投影到目标词汇表空间，得到logits。
        5.  Logits经过softmax函数，转换为每个词的概率，用于预测下一个词。

2.  **设计顶层 `Transformer` 类**
    *   这个类将是所有组件的总指挥。它需要将数据流正确地引导到编码器和解码器。
    *   **`__init__`**:
        *   实例化一个 `Encoder`。
        *   实例化一个 `Decoder`。
        *   实例化一个最终的线性层 (`nn.Linear`)。这个线性层的输入维度是 `d_model`，输出维度是目标词汇表的大小 `tgt_vocab_size`。这个层通常被称为“输出投影层”或“生成器”。
        *   **权重共享（Weight Sharing）**：论文中提到了一个重要的优化技巧：输出投影层的权重可以与目标词汇的嵌入层（`Decoder`内部的`embedding`层）的权重共享。这样做有两个好处：
            1.  **减少参数量**：在词汇表很大时，嵌入层和输出层的参数量是模型中最大的部分。共享权重可以显著减少模型大小。
            2.  **正则化效果**：它在输入和输出之间建立了一种对称关系，直观上讲，一个词的“嵌入表示”（从ID到向量）和它的“预测表示”（从向量到logits）应该是有联系的。共享权重强制了这种联系。我将在实现中加入这个技巧。
    *   **`forward`**:
        *   该方法需要接收 `src` (源序列IDs) 和 `tgt` (目标序列IDs)。
        *   **创建掩码**：这是个关键且容易出错的地方。我需要在这个顶层方法中，根据输入数据（特别是padding token）统一生成所有需要的掩码。
            *   `src_mask`: 源序列的padding mask。
            *   `tgt_mask`: 目标序列的组合掩码，既要处理padding，也要处理未来的信息泄露（causal mask）。
        *   **数据流**:
            1.  调用 `self.encoder`，传入 `src` 和 `src_mask`，得到 `memory`。
            2.  调用 `self.decoder`，传入 `tgt`, `memory`, `tgt_mask`, 和 `src_mask`，得到解码器输出。
            3.  将解码器输出送入最终的线性层，得到 `logits`。
            4.  返回 `logits`。softmax通常在计算损失函数时（如 `nn.CrossEntropyLoss`）内部处理，所以模型本身通常只返回logits。

3.  **实现掩码生成**
    *   为了让模型更易用，我决定在`Transformer`类中写一个辅助方法 `_create_masks`。这个方法接收`src`和`tgt`张量，并返回所有需要的掩码。
    *   `src_mask` 的创建很简单：找到`src`中不等于`pad_idx`的位置即可。
    *   `tgt_mask` 的创建稍复杂：
        1.  先创建`tgt`的padding mask。
        2.  再创建一个与`tgt`序列长度相同的下三角因果掩码。
        3.  将这两个掩码用逻辑与（`&`）操作合并，得到最终的`tgt_mask`。

#### **为什么这么实现**

*   **高层封装**：将所有子模块封装在一个`Transformer`类中，为用户提供了一个非常简洁的接口。用户只需要关心输入源序列和目标序列，而不需要处理内部复杂的掩码生成和数据流转。
*   **最佳实践**：包含了权重共享这一重要的实践技巧，这在现代Transformer实现中非常普遍。将掩码创建逻辑封装在模型内部，也减少了用户使用模型时出错的可能性。
*   **端到端**：这个实现最终构成了一个完整的、从输入ID到输出概率（通过logits体现）的端到端模型，可以直接用于训练和推理。

#### **代码实现 (Part 6: The Final Transformer Model)**

```python
import torch
import torch.nn as nn
import math
import copy

# --- 从前面部分复制过来的完整模块 ---
# (为了保持代码的完整性和可运行性，这里包含了所有之前定义的类)
def _clone_modules(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model, self.n_heads, self.d_k = d_model, n_heads, d_model // n_heads
        self.w_q, self.w_k, self.w_v, self.w_o = (nn.Linear(d_model, d_model) for _ in range(4))
        self.dropout = nn.Dropout(dropout)
    def forward(self, q, k, v, mask=None):
        bs = q.size(0)
        q, k, v = [l(x).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2) for l, x in zip((self.w_q, self.w_k, self.w_v), (q, k, v))]
        def scaled_dot_product_attention(q, k, v, mask=None):
            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
            if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)
            p_attn = torch.softmax(scores, dim=-1)
            return torch.matmul(p_attn, v), p_attn
        context, attn = scaled_dot_product_attention(q, k, v, mask)
        context = context.transpose(1, 2).contiguous().view(bs, -1, self.d_model)
        return self.w_o(context), attn

class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.w_2(self.dropout(self.relu(self.w_1(x))))

class EncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        self.norm1, self.norm2 = (nn.LayerNorm(d_model) for _ in range(2))
        self.dropout1, self.dropout2 = (nn.Dropout(dropout) for _ in range(2))
    def forward(self, src, src_mask):
        attn_output, _ = self.self_attn(src, src, src, src_mask)
        src = self.norm1(src + self.dropout1(attn_output))
        ffn_output = self.feed_forward(src)
        src = self.norm2(src + self.dropout2(ffn_output))
        return src

class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)
        layer = EncoderLayer(d_model, n_heads, d_ff, dropout)
        self.layers = _clone_modules(layer, n_layers)
        self.norm = nn.LayerNorm(d_model)
        self.d_model = d_model
    def forward(self, src, src_mask):
        src_emb = self.embedding(src) * math.sqrt(self.d_model)
        src_pos = self.pos_encoder(src_emb)
        output = src_pos
        for layer in self.layers:
            output = layer(output, src_mask)
        return self.norm(output)

class DecoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        self.norm1, self.norm2, self.norm3 = (nn.LayerNorm(d_model) for _ in range(3))
        self.dropout1, self.dropout2, self.dropout3 = (nn.Dropout(dropout) for _ in range(3))
    def forward(self, tgt, memory, tgt_mask, src_mask):
        attn_output, _ = self.self_attn(tgt, tgt, tgt, tgt_mask)
        tgt = self.norm1(tgt + self.dropout1(attn_output))
        cross_attn_output, _ = self.cross_attn(tgt, memory, memory, src_mask)
        tgt = self.norm2(tgt + self.dropout2(cross_attn_output))
        ffn_output = self.feed_forward(tgt)
        tgt = self.norm3(tgt + self.dropout3(ffn_output))
        return tgt

class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)
        layer = DecoderLayer(d_model, n_heads, d_ff, dropout)
        self.layers = _clone_modules(layer, n_layers)
        self.norm = nn.LayerNorm(d_model)
        self.d_model = d_model
    def forward(self, tgt, memory, tgt_mask, src_mask):
        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)
        tgt_pos = self.pos_encoder(tgt_emb)
        output = tgt_pos
        for layer in self.layers:
            output = layer(output, memory, tgt_mask, src_mask)
        return self.norm(output)

# --------------------------------------------------------------------------------
# Part 6: The Full Transformer Model
# --------------------------------------------------------------------------------
class Transformer(nn.Module):
    """
    完整的Transformer模型。
    思想过程：
    1.  将Encoder和Decoder组合起来。
    2.  添加一个最终的线性层来生成词汇表大小的logits。
    3.  实现权重共享：解码器的嵌入层和最终的线性层共享权重。
    4.  在一个统一的forward方法中处理整个流程，包括掩码的创建。
    """
    def __init__(self, 
                 src_vocab_size: int, 
                 tgt_vocab_size: int, 
                 d_model: int, 
                 n_layers: int, 
                 n_heads: int, 
                 d_ff: int, 
                 dropout: float = 0.1, 
                 max_len: int = 5000,
                 pad_idx: int = 0):
        super().__init__()
        
        self.encoder = Encoder(src_vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len)
        self.decoder = Decoder(tgt_vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len)
        self.generator = nn.Linear(d_model, tgt_vocab_size)
        
        # 权重共享
        self.decoder.embedding.weight = self.generator.weight

        self.pad_idx = pad_idx
        
        # 初始化权重
        self._initialize_weights()

    def _initialize_weights(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def _create_masks(self, src: torch.Tensor, tgt: torch.Tensor):
        # 源序列padding mask
        # src_mask: [B, 1, 1, S_seq]
        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)

        # 目标序列padding mask
        # tgt_pad_mask: [B, 1, 1, T_seq]
        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)
        
        # 目标序列因果掩码 (look-ahead mask)
        tgt_len = tgt.size(1)
        # look_ahead_mask: [1, 1, T_seq, T_seq]
        look_ahead_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=src.device)).bool().unsqueeze(0).unsqueeze(0)

        # 合并目标序列的两种掩码
        # tgt_mask: [B, 1, T_seq, T_seq]
        tgt_mask = tgt_pad_mask & look_ahead_mask
        
        return src_mask, tgt_mask

    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:
        # src: [B, S_seq], tgt: [B, T_seq]
        
        # 1. 创建掩码
        src_mask, tgt_mask = self._create_masks(src, tgt)

        # 2. 编码器前向传播
        memory = self.encoder(src, src_mask)
        
        # 3. 解码器前向传播
        decoder_output = self.decoder(tgt, memory, tgt_mask, src_mask)
        
        # 4. 生成logits
        logits = self.generator(decoder_output)
        
        return logits # shape: [B, T_seq, tgt_vocab_size]

# --- 测试一下 ---
if __name__ == '__main__':
    print("--- Part 6: Testing the Full Transformer Model ---")
    # 假设参数
    src_vocab_size = 1000
    tgt_vocab_size = 1200
    d_model = 512
    n_layers = 6
    n_heads = 8
    d_ff = 2048
    batch_size = 4
    src_seq_len = 20
    tgt_seq_len = 18
    pad_idx = 0
    
    # 创建模型实例
    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, n_layers, n_heads, d_ff, pad_idx=pad_idx).to('cpu')
    
    # 随机生成输入
    src = torch.randint(1, src_vocab_size, (batch_size, src_seq_len))
    tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len))
    
    # 模拟padding
    src[0, -5:] = pad_idx
    tgt[0, -3:] = pad_idx
    
    # 前向传播
    logits = model(src, tgt)
    
    print("Input src shape:", src.shape)
    print("Input tgt shape:", tgt.shape)
    print("Output logits shape:", logits.shape)
    
    # 检查输出维度
    assert logits.shape == (batch_size, tgt_seq_len, tgt_vocab_size)
    
    print("\nFull Transformer model test passed!")
    
    # 检查权重共享
    assert torch.equal(model.decoder.embedding.weight, model.generator.weight)
    print("Weight sharing between decoder embedding and generator is verified.")
```

### **实现总结与回顾**

我们从最底层的`scaled_dot_product_attention`函数开始，一步步向上构建，最终完成了整个Transformer模型。回顾这个过程，我的实现思路始终遵循以下原则：

1.  **自底向上，逐步验证**：从最小的功能单元开始，确保其正确性，然后再用它来构建更复杂的模块。每完成一个大模块，都编写一个简单的测试用例来验证其输入输出的形状是否符合预期。
2.  **模块化与封装**：将不同的功能（如注意力、FFN、位置编码、编码器层、解码器层）封装在独立的类中。这使得代码结构清晰，易于理解、调试和复用。
3.  **忠于原文与采纳实践**：实现的核心逻辑严格遵循《Attention Is All You Need》论文的描述，同时采纳了社区的最佳实践，如`sqrt(d_model)`缩放嵌入、权重共享和Xavier初始化等，这些都是让模型更好训练的关键细节。
4.  **关注细节，尤其是张量操作和掩码**：Transformer的实现中充满了对张量形状的变换（`view`, `transpose`, `contiguous`）。清晰地追踪这些变化并添加注释至关重要。同时，正确地创建和应用两种不同的掩码（padding mask 和 causal mask）是模型能够正确工作的核心。

现在，我们有了一个完整的、基于PyTorch的Transformer模型。下一步就是准备数据、定义损失函数（`nn.CrossEntropyLoss`，记得忽略`pad_idx`）、选择优化器（如Adam），然后就可以开始训练它来完成机器翻译或其他序列到序列的任务了。
好的，我们继续。在上一步中，我们构建了Transformer的核心计算引擎——缩放点积注意力。现在，我们要将这个核心引擎封装成一个更强大、更灵活的模块：**多头注意力（Multi-Head Attention）**。

---

### **第二部分：多头注意力（Multi-Head Attention）**

#### **思想过程**

1.  **为什么需要多头？**
    *   单一的注意力机制（像我们上一步实现的）强迫模型在所有信息中只学习一种“关注模式”。这是一种限制。
    *   论文的作者们设想，如果能让模型从不同的“角度”或“子空间”去审视输入序列，效果会不会更好？比如，一个“头”可能关注句法依赖（“the”后面常跟名词），另一个“头”可能关注语义关联（“eat”和“apple”），还有一个“头”可能关注长距离依赖关系。
    *   多头注意力就是实现这一思想的机制。它将原始的`Query`, `Key`, `Value` 通过线性变换，投影到多个不同的、低维度的子空间中，然后在每个子空间并行地执行注意力计算，最后将所有“头”的结果拼接并再次进行线性变换，得到最终的输出。

2.  **如何从单头到多头？**
    *   **分解（Projection）**: 假设我们有 `h` 个头，模型的维度是 `d_model`。我们会将输入 `Q, K, V`（维度都是 `d_model`）分别通过三个独立的线性层，将它们投影成 `h` 组新的、维度更低的 `q_i, k_i, v_i`。每个头处理的向量维度是 `d_k = d_v = d_model / h`。
        *   **实现细节**: 我可以在代码里创建 `h` 组独立的线性层，但这很笨拙且效率低下。一个更优雅、更高效的方法是：创建一个大的线性层，其输出维度仍然是 `d_model`，然后将这个 `d_model` 维度的输出向量在逻辑上“切分”成 `h` 个 `d_k` 维度的块。这可以通过简单的张量变形（`reshape` 或 `view`）和维度交换（`transpose`）来高效完成。这利用了GPU并行计算的优势。
    *   **并行计算注意力（Parallel Attention）**: 对这 `h` 组 `(q_i, k_i, v_i)`，我们分别调用上一步实现的 `scaled_dot_product_attention` 函数。由于我们的张量已经被塑造成 `[batch_size, n_heads, seq_len, d_k]` 的形式，这个并行计算可以一次性完成，非常高效。
    *   **合并（Concatenation & Final Projection）**: 将 `h` 个头的输出结果（`head_i`）拼接起来。因为每个头的输出维度是 `d_v`，拼接后的维度是 `h * d_v`。根据论文的设计 `d_v = d_k = d_model / h`，所以拼接后的维度又变回了 `d_model`。最后，将这个拼接后的向量再通过一个线性层（`W^O`）进行一次最终的变换和融合。这一步允许模型学习如何最好地组合来自不同头的信息。

3.  **构建 `nn.Module`**
    *   这个模块显然需要有可学习的参数（`W^Q`, `W^K`, `W^V`, `W^O` 这四个线性变换矩阵），所以它必须是一个 `nn.Module` 的子类。
    *   在 `__init__` 中，我需要定义这四个核心的 `nn.Linear` 层。我还将定义 `d_model`, `n_heads` 和 `d_k` 等超参数。一个好的实践是加入一个断言 `assert d_model % n_heads == 0`，确保维度可以被整除，防止配置错误。
    *   在 `forward` 方法中，我将实现上述的“分解 -> 并行计算 -> 合并”的完整流程。这里的关键是张量的维度操作，我会写清楚注释，追踪`[batch, seq, model]` -> `[batch, seq, heads, d_k]` -> `[batch, heads, seq, d_k]` -> `[batch, seq, model]`的整个变换过程。
    *   我还将在这里加入论文中提到的`Dropout`，它被应用在子层的输出上，以增强模型的泛化能力。

#### **为什么这么实现**

*   **封装性**：将多头注意力的所有逻辑封装在一个独立的 `MultiHeadAttention` 类中，使得代码结构清晰，易于复用。编码器和解码器都可以直接调用这个模块。
*   **效率**：通过使用一个大的线性层然后进行`reshape`和`transpose`，而不是使用多个小的线性层，可以最大化地利用GPU的并行计算能力，使模型训练和推理更快。这是PyTorch中实现多头注意力的标准和高效方式。
*   **忠于原文**：这个实现严格遵循了论文中图2的结构，包含了输入线性投影、并行注意力计算、结果拼接和输出线性投影这几个关键步骤。

#### **代码实现 (Part 2: Multi-Head Attention)**

```python
import torch
import torch.nn as nn
import math

# --- 从第一部分复制过来的函数 ---
def scaled_dot_product_attention(q, k, v, mask=None):
    d_k = k.size(-1)
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = torch.softmax(scores, dim=-1)
    return torch.matmul(p_attn, v), p_attn

# --------------------------------------------------------------------------------
# Part 2: Multi-Head Attention
# --------------------------------------------------------------------------------

class MultiHeadAttention(nn.Module):
    """
    多头注意力机制模块。

    思想过程：
    1.  初始化：
        - 接收 d_model (模型总维度) 和 n_heads (头的数量)。
        - 确保 d_model 可以被 n_heads 整除。
        - 定义 d_k, d_v 为每个头的维度。
        - 创建四个关键的线性层：W_q, W_k, W_v 用于输入投影，W_o 用于输出投影。
          我们使用一个大的 nn.Linear(d_model, d_model) 来一次性完成所有头的投影，
          这比创建 n_heads 个小的线性层更高效。
        - 创建一个dropout层。
    2.  前向传播 (forward):
        - 接收 query, key, value 和 mask。
        - 记录 batch_size。
        - 将 q, k, v 分别通过 W_q, W_k, W_v 线性层进行投影。
        - 对投影后的结果进行 reshape 和 transpose，从 [batch, seq_len, d_model] 变为
          [batch, n_heads, seq_len, d_k]，为并行计算做准备。
        - 调用之前实现的 scaled_dot_product_attention 函数，传入多头形式的 q, k, v 和 mask。
        - 将注意力计算的输出结果 reshape 回 [batch, seq_len, d_model] 的形式。
          这一步相当于论文中的 "Concat"。
        - 将合并后的结果通过最后的 W_o 线性层进行输出投影。
        - 应用 dropout。
    """
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"

        self.d_model = d_model
        self.n_heads = n_heads
        # d_k 是每个头的维度
        self.d_k = d_model // n_heads

        # 定义四个线性层
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, 
                q: torch.Tensor, 
                k: torch.Tensor, 
                v: torch.Tensor, 
                mask: torch.Tensor = None) -> (torch.Tensor, torch.Tensor):
        
        batch_size = q.size(0)

        # 1. 线性投影
        # q, k, v: [batch_size, seq_len, d_model]
        q_proj = self.w_q(q)
        k_proj = self.w_k(k)
        v_proj = self.w_v(v)

        # 2. Reshape 和 Transpose 以便进行多头注意力计算
        # from: [batch_size, seq_len, d_model]
        # to:   [batch_size, n_heads, seq_len, d_k]
        q_proj = q_proj.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        k_proj = k_proj.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        v_proj = v_proj.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 3. 并行计算注意力
        # context: [batch_size, n_heads, seq_len_q, d_k]
        # attn: [batch_size, n_heads, seq_len_q, seq_len_k]
        context, attn = scaled_dot_product_attention(q_proj, k_proj, v_proj, mask)

        # 4. Concat: Reshape 和 Transpose 回来
        # from: [batch_size, n_heads, seq_len_q, d_k]
        # to:   [batch_size, seq_len_q, d_model]
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # 5. 输出投影
        output = self.w_o(context)
        output = self.dropout(output)

        return output, attn

# --- 测试一下 ---
if __name__ == '__main__':
    print("--- Part 2: Testing Multi-Head Attention ---")
    # 假设参数
    d_model = 512
    n_heads = 8
    batch_size = 4
    seq_len = 10
    
    # 创建多头注意力模块实例
    mha = MultiHeadAttention(d_model=d_model, n_heads=n_heads)
    
    # 随机生成输入 (在自注意力中, q, k, v是相同的)
    x = torch.randn(batch_size, seq_len, d_model)
    
    # 创建一个padding mask (假设后两个token是padding)
    # mask shape: [batch_size, 1, 1, seq_len_k]
    padding_mask = torch.ones(batch_size, seq_len)
    padding_mask[:, -2:] = 0
    padding_mask = padding_mask.unsqueeze(1).unsqueeze(1) # for broadcasting with attention scores
    
    print("Input shape:", x.shape)
    print("Padding mask shape:", padding_mask.shape)
    
    # 前向传播
    output, attn_weights = mha(q=x, k=x, v=x, mask=padding_mask)
    
    print("\nOutput shape:", output.shape)
    print("Attention weights shape:", attn_weights.shape)
    
    # 检查输出维度是否正确
    assert output.shape == (batch_size, seq_len, d_model)
    # 检查注意力权重维度是否正确
    assert attn_weights.shape == (batch_size, n_heads, seq_len, seq_len)
    
    print("\nMulti-Head Attention module test passed!")
    
    # 打印一个头的注意力权重，看看padding mask是否生效
    print("Attention weights for one head (first sequence in batch):")
    print("Note that the last two columns are all zeros, indicating the padding mask is working.")
    print(attn_weights[0, 0].round(decimals=3))

```

#### **总结与下一步**

我们成功地将缩放点积注意力封装成了一个功能强大的多头注意力模块。这是Transformer中一个至关重要的可复用组件。

现在我们有了“注意力”这个核心工具，但一个完整的Transformer层不仅仅有注意力。回顾论文中的架构图，每个子层（无论是多头注意力还是前馈网络）的输出都经过了一个**残差连接（Residual Connection）**，然后是一个**层归一化（Layer Normalization）**。这个 `Add & Norm` 的步骤对于训练深度Transformer模型至关重要。

**因此，我的下一步是：实现前馈神经网络（Position-wise Feed-Forward Network），并将它和多头注意力模块组合起来，通过残差连接和层归一化，构建一个完整的编码器层（Encoder Layer）。**

我将在下一次回答中继续这个过程。